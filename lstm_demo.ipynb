{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 44981\n",
      "vectorization...\n",
      "0\n",
      "30000\n",
      "Epoch 1/20\n",
      "175/175 [==============================] - 238s - loss: 8.3286   \n",
      "Epoch 2/20\n",
      "175/175 [==============================] - 231s - loss: 7.0119   \n",
      "Epoch 3/20\n",
      "175/175 [==============================] - 231s - loss: 7.0095   \n",
      "Epoch 4/20\n",
      "175/175 [==============================] - 231s - loss: 6.9613   \n",
      "Epoch 5/20\n",
      "175/175 [==============================] - 231s - loss: 6.9561   \n",
      "Epoch 6/20\n",
      "175/175 [==============================] - 231s - loss: 6.9236   \n",
      "Epoch 7/20\n",
      "175/175 [==============================] - 231s - loss: 6.9447   \n",
      "Epoch 8/20\n",
      "175/175 [==============================] - 231s - loss: 6.9940   \n",
      "Epoch 9/20\n",
      "175/175 [==============================] - 231s - loss: 6.9796   \n",
      "Epoch 10/20\n",
      "175/175 [==============================] - 231s - loss: 6.9919   \n",
      "Epoch 11/20\n",
      "175/175 [==============================] - 231s - loss: 7.0645   \n",
      "Epoch 12/20\n",
      "175/175 [==============================] - 231s - loss: 7.1718   \n",
      "Epoch 13/20\n",
      "175/175 [==============================] - 231s - loss: 7.1861   \n",
      "Epoch 14/20\n",
      "175/175 [==============================] - 231s - loss: 7.1194   \n",
      "Epoch 15/20\n",
      "175/175 [==============================] - 231s - loss: 7.1803   \n",
      "Epoch 16/20\n",
      "175/175 [==============================] - 231s - loss: 7.2163   \n",
      "Epoch 17/20\n",
      "175/175 [==============================] - 231s - loss: 7.2113   \n",
      "Epoch 18/20\n",
      "175/175 [==============================] - 231s - loss: 7.2591   \n",
      "Epoch 19/20\n",
      "175/175 [==============================] - 231s - loss: 7.2389   \n",
      "Epoch 20/20\n",
      "175/175 [==============================] - 231s - loss: 7.1805   \n"
     ]
    }
   ],
   "source": [
    "# %load lstm_prob.py\n",
    "\"\"\"\n",
    "Created on Mon Nov 13 19:53:19 2017\n",
    "\n",
    "@author: merry jane\n",
    "\n",
    "1、训练lstm 生成文本模型\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "import numpy as np\n",
    "#from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "import re\n",
    "import jieba\n",
    "import pickle\n",
    "import data_helper\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_lstm_model(batch_size=256):\n",
    "    '''\n",
    "    arg:\n",
    "        char_indices:输入文本的词语索引字典\n",
    "        maxlen:每一行文本按词语最大长度\n",
    "        \n",
    "    \n",
    "    模型如图所示，利用keras搭建，tensorflow作为backend\n",
    "    ________________________________________________________________\n",
    "    Layer (type)                 Output Shape              Param #   \n",
    "    =================================================================\n",
    "    lstm_24 (LSTM)               (None, 256)               13986816  \n",
    "    _________________________________________________________________\n",
    "    dense_14 (Dense)             (None, 13402)             3444314   \n",
    "    _________________________________________________________________\n",
    "    activation_14 (Activation)   (None, 13402)             0         \n",
    "    =================================================================\n",
    "    Total params: 17,431,130\n",
    "    Trainable params: 17,431,130\n",
    "    Non-trainable params: 0\n",
    "    _________________________________________________________________\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    batch_size=batch_size\n",
    "    #print ('build model...')\n",
    "    model=Sequential()\n",
    "    model.add(LSTM(256,input_shape=(maxlen,len(char_indices)),recurrent_dropout=0.1,dropout=0.1))\n",
    "    model.add(Dense(len(char_indices)))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    optimizer=keras.optimizers.RMSprop(lr=0.01)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizer)\n",
    "    \n",
    "    #print (model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(X,y,batch_size=256,epochs=10):\n",
    "    '''\n",
    "    小批量训练fit_generator\n",
    "    '''\n",
    "    model.fit_generator(data_generator(X,y,batch_size),steps_per_epoch=X.shape[0]//batch_size,epochs=epochs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def data_generator(X,y,batch_size):\n",
    "    '''\n",
    "    数据较小批量生成\n",
    "    '''\n",
    "    if batch_size<1:\n",
    "        batch_size=256\n",
    "    number_of_batchs=X.shape[0]/batch_size\n",
    "    counter=0\n",
    "    shuffle_index=np.arange(np.shape(y)[0])\n",
    "    np.random.shuffle(shuffle_index)\n",
    "    \n",
    "    #reset generator\n",
    "    while 1:\n",
    "        index_batch=shuffle_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch=(X[index_batch,:,:]).astype('float32')\n",
    "        y_batch=(y[index_batch,:]).astype('float32')\n",
    "        counter+=1\n",
    "        yield(np.array(X_batch),y_batch)\n",
    "        \n",
    "        if (counter<number_of_batchs):\n",
    "            np.random.shuffle(shuffle_index)\n",
    "            counter=0\n",
    "            \n",
    "            \n",
    "def gen_matrix_for_sentence():\n",
    "    #对构造句子 对其矩阵化\n",
    "    print ('vectorization...')\n",
    "    X=np.zeros((len(sentences),maxlen,len(char_indices)),dtype=np.bool)\n",
    "    y=np.zeros((len(sentences),len(char_indices)),dtype=np.bool)\n",
    "    \n",
    "    for i,sentence in enumerate(sentences[:]):\n",
    "        if (i%30000==0):\n",
    "            print (i)\n",
    "            for t in range(maxlen):\n",
    "                char_index=sentence[t]\n",
    "                X[i,t,char_index]=1\n",
    "    \n",
    "        y[i,next_chars[i]]=1\n",
    "    \n",
    "    return X,y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "if __name__=='__main__':\n",
    "    \n",
    "    #1、读文章数据\n",
    "    #############alltext只要是文本即可 也可以留下文本标点###################\n",
    "#    f=open('./fxhh_article.csv','r',encoding='utf-8')\n",
    "#    line=f.readline()\n",
    "#   alltext=''\n",
    "#    i=0\n",
    "#    while True:\n",
    "#        line=f.readline()\n",
    "#        if not line:break\n",
    "#        i+=1\n",
    "        \n",
    "        #剔除标点符号\n",
    "#        alltext+=line.split('\\x01')[2]+\" \"+\" \".join([x.strip() for x in re.findall('(.*?)[|,|;|?|.|!|:|，|。|、|~|；|！|？|\"|“|”|】|【|《|》|<|>|&|%|$|(|)|（|）|^|#|@]', line.split('\\x01')[3] + \",\")])\n",
    "        \n",
    "        #防止内存溢出 在gen_matrix_for_sentence\n",
    "#        if i>1000:break\n",
    "    \n",
    "    f=open('./7fresh_train_data_fruit.txt','r',encoding='utf-8')   \n",
    "    alltext=''\n",
    "    i=0\n",
    "    while True:\n",
    "        line=f.readline()\n",
    "        if not line:break\n",
    "        i+=1\n",
    "        \n",
    "\n",
    "        alltext+=f.readline().replace(\" \",\"\").strip()+\" \"       \n",
    "        #防止内存溢出 在gen_matrix_for_sentence\n",
    "        if i>15000:break\n",
    "    ###############################################################\n",
    "    \n",
    "    #2、生成词与索引，索引与词的映射字典\n",
    "    data_helper.preprocess_and_save_data(alltext, data_helper.create_lookup_tables) #生成并保存\n",
    "    int_text, char_indices, indices_char = data_helper.load_preprocess() #加载 int_text即是包含所有词语索引号的数组\n",
    "    \n",
    "    #3、构造句子序列 int_text, vocab_to_int, int_to_vocab\n",
    "    maxlen=4 #每个句子包含词语的最大个数\n",
    "    step=2 #跳跃生成\n",
    "    sentences=[]\n",
    "    next_chars=[]\n",
    "    for i in range(0,len(int_text)-maxlen,step):\n",
    "        sentences.append(int_text[i:i+maxlen])\n",
    "        next_chars.append(int_text[i+maxlen])\n",
    "    \n",
    "    print ('nb sequences:',len(sentences))\n",
    "\n",
    "    #4、生成句子转矩阵  \n",
    "    X,y=gen_matrix_for_sentence()\n",
    "     \n",
    "    #5、bulid模型   默认batch_size=256 小批量处理的句子树\n",
    "    model=build_lstm_model(batch_size=256)\n",
    "    \n",
    "    #6、训练模型 epochs为训练轮次\n",
    "    train_model(X,y,batch_size=256,epochs=20)\n",
    "    \n",
    "    \n",
    "    #7、保存模型\n",
    "    model.save('./lstm_model_gen_word_epoch_20.h5')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f=open('./7fresh_train_data_fruit.txt','r',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'据说是汉朝张骞出使西域时由中亚经丝绸之路带入我国的 葡萄的含糖量达8%～10% 维生素及多种具有生理功能的物质 葡萄中的糖易被人体吸收 能很快被人体吸收 葡萄能比阿斯匹林更好地阻止血栓形成 降低血小板的凝聚力 葡萄中含的类黄酮是一种强力抗氧化剂 并可清除体内自由基 有帮助消化的作用 对人体裨益甚大 葡萄中含有一种有益的抗癌物质 并能防止癌细胞扩散 促进早日康复 葡萄性平味甘 生津液 通利小便的作用'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alltext[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f=open('./7fresh_train_data_fruit.txt','r',encoding='utf-8')   \n",
    "alltext=''\n",
    "i=0\n",
    "while True:\n",
    "    line=f.readline()\n",
    "    if not line:break\n",
    "    i+=1\n",
    "\n",
    "\n",
    "    alltext+=f.readline().replace(\" \",\"\").strip()+\" \"       \n",
    "\n",
    "\n",
    "#2、生成词与索引，索引与词的映射字典\n",
    "data_helper.preprocess_and_save_data(alltext, data_helper.create_lookup_tables) #生成并保存\n",
    "int_text, char_indices, indices_char = data_helper.load_preprocess() #加载 int_te\n",
    "\n",
    "\n",
    "wf=open(\"sentence_index.csv\",\"w\")\n",
    "\n",
    "maxlen=10 #每个句子包含词语的最大个数\n",
    "step=3 #跳跃生成\n",
    "sentences=[]\n",
    "next_chars=[]\n",
    "for i in range(0,len(int_text)-maxlen,step):\n",
    "    wf.write(str(int_text[i:i+maxlen]).strip(\"[]\")+\"\\t\"+str(int_text[i+maxlen])+\"\\n\")\n",
    "\n",
    "wf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "import numpy as np\n",
    "#from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "import re\n",
    "import jieba\n",
    "import pickle\n",
    "import data_helper\n",
    "\n",
    "int_text, char_indices, indices_char = data_helper.load_preprocess() #加载 int_te\n",
    "def process_line(line):  \n",
    "    tmp_x = [int(val) for val in line.strip().split('\\t')[0].split(\",\")]\n",
    "    tmp_y=int(line.strip().split('\\t')[1])\n",
    "    \n",
    "    x=np.zeros((maxlen,len(char_indices)),dtype=float)\n",
    "    y=np.zeros(len(char_indices),dtype=float)\n",
    "   \n",
    "    for t in range(maxlen):\n",
    "        char_index=tmp_x[t]\n",
    "        x[t,char_index]=1\n",
    "    \n",
    "    y[tmp_y]=1\n",
    "\n",
    "\n",
    "    return x,y  \n",
    "\n",
    "def generate_arrays_from_file(path,batch_size):  \n",
    "    while 1:  \n",
    "        f = open(path)  \n",
    "        cnt = 0  \n",
    "        X =[]  \n",
    "        Y =[]  \n",
    "        for line in f:  \n",
    "            # create Numpy arrays of input data  \n",
    "            # and labels, from each line in the file  \n",
    "            x, y = process_line(line)  \n",
    "            X.append(x)  \n",
    "            Y.append(y)\n",
    "        \n",
    "            cnt += 1  \n",
    "            if cnt==batch_size:  \n",
    "                cnt = 0  \n",
    "                yield (np.array(X), np.array(Y)) \n",
    "\n",
    "                \n",
    "                X = []  \n",
    "                Y = []\n",
    "\n",
    "    f.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 256)               44402688  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 43105)             11077985  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 43105)             0         \n",
      "=================================================================\n",
      "Total params: 55,480,673\n",
      "Trainable params: 55,480,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "maxlen=10\n",
    "def build_lstm_model(batch_size=256):\n",
    "\n",
    "    batch_size=batch_size\n",
    "    #print ('build model...')\n",
    "    model=Sequential()\n",
    "    model.add(LSTM(256,input_shape=(maxlen,len(char_indices)),recurrent_dropout=0.1,dropout=0.1))\n",
    "    model.add(Dense(len(char_indices)))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    optimizer=keras.optimizers.RMSprop(lr=0.01)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizer)\n",
    "    \n",
    "    #print (model.summary())\n",
    "    \n",
    "    return model\n",
    "model=build_lstm_model(batch_size=100)\n",
    "print (model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianxiaorong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/jianxiaorong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., steps_per_epoch=1000, epochs=20, verbose=1, workers=1)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "   5/1000 [..............................] - ETA: 22335s - loss: 6.4309"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianxiaorong/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (2.703873). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2495s - loss: 6.3062  \n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 2392s - loss: 8.3037  \n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 2394s - loss: 8.6284  \n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 2395s - loss: 8.9761  \n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 2395s - loss: 8.2446  \n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 2395s - loss: 8.5986  \n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 2395s - loss: 8.4997  \n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 2394s - loss: 8.6378  \n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 2394s - loss: 7.9685  \n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 2395s - loss: 8.2732  \n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 2396s - loss: 8.1703  \n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 2396s - loss: 8.1942  \n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 2397s - loss: 7.6104  \n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 2402s - loss: 7.9718  \n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 2396s - loss: 7.8405  \n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 2396s - loss: 7.8469  \n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 2397s - loss: 7.2923  \n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 2397s - loss: 7.6675  \n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 2396s - loss: 7.5405  \n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 2396s - loss: 7.5306  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fed0525fcc0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generate_arrays_from_file(\"./sentence_index.csv\",50),steps_per_epoch=1000,epochs=20,verbose=1,nb_worker=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('./lstm_model_gen_word_epoch_20.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# to check the pydot/graphviz installation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'Dot'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-437440fcd987>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \"\"\"\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rankdir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# pydot raises a generic Exception here,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# so no specific class can be caught.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         raise ImportError('Failed to import pydot. You must install pydot'\n\u001b[0m\u001b[1;32m     28\u001b[0m                           ' and graphviz for `pydotprint` to work.')\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work."
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
